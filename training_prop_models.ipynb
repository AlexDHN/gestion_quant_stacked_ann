{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Ann import Ann\n",
    "from classes.DataLoader import DataLoader\n",
    "from classes.DataLoader_batch import DataLoader_batch\n",
    "from classes.Simulation import Simulation\n",
    "\n",
    "import sklearn\n",
    "from classes.GradientBoosting import GradientBoosting as GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from classes.RandomForestClassifier import RandomForest as RandomForestRegressor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from utils.init_functions import random_normal\n",
    "from classes.garchpq import GARCH\n",
    "from arch import arch_model\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"S&P500\"\n",
    "data = pd.read_csv(\"data/dataset.csv\", index_col=0)\n",
    "data.index = pd.to_datetime(data.index)\n",
    "df_trv = data.rolling(5).std()\n",
    "df_trv.index = pd.to_datetime(df_trv.index)\n",
    "df_trv = df_trv.iloc[4:]\n",
    "dfe = df_trv[index].dropna() \n",
    "mi = dfe.min()\n",
    "ma = dfe.max()\n",
    "dfe = (dfe - mi)/(ma-mi)\n",
    "dfe_r = data[\"S&P500\"][dfe.index]\n",
    "\n",
    "am = GARCH()\n",
    "res = am.fit(dfe_r.values, [dfe_r.values.mean(), dfe_r.values.mean().var() * 0.01, 0.078, 0.815])\n",
    "dfe_garch = pd.Series(np.sqrt(am.sigma2), index=dfe_r.index)\n",
    "mi_garch = dfe_garch.min()\n",
    "ma_garch = dfe_garch.max()\n",
    "dfe_garch = (dfe_garch - mi_garch)/(ma_garch-mi_garch)\n",
    "\n",
    "am = arch_model(dfe_r.values, vol='EGARCH' , p = 1 , o = 0 , q = 1)\n",
    "res = am.fit(disp='off')\n",
    "dfe_egarch = pd.Series(res.conditional_volatility, index=dfe_r.index)\n",
    "mi_egarch = dfe_egarch.min()\n",
    "ma_egarch = dfe_egarch.max()\n",
    "df_egarch = (dfe_egarch - mi_egarch)/(ma_egarch-mi_egarch)\n",
    "\n",
    "periods = {\n",
    "    \"2000-2007\":\"2008\",\n",
    "    \"2001-2008\":\"2009\",\n",
    "    \"2002-2009\":\"2010\",\n",
    "    \"2009-2016\":\"2017\",\n",
    "    \"2010-2017\":\"2018\",\n",
    "}\n",
    "\n",
    "params_stacked = {\n",
    "    \"2000-2007\":[0.0033,0],\n",
    "    \"2001-2008\":[0.0059, 0.01],\n",
    "    \"2002-2009\":[0.0136, 0],\n",
    "    \"2009-2016\":[0.085, 0.02],\n",
    "    \"2010-2017\":[0.01, 0.011],\n",
    "}\n",
    "\n",
    "params_ml = {\n",
    "    \"2000-2007\":[10, 24, 1479, 0.003, 0.0001, 0.45],\n",
    "    \"2001-2008\":[10, 107, 3000, 0.001, 0.0001, 0.55],\n",
    "    \"2002-2009\":[1, 37, 3583, 0.001, 0.0004, 0.17],\n",
    "    \"2009-2016\":[30, 118, 1000, 0.009, 0.0002, 0.13],\n",
    "    \"2010-2017\":[7, 175, 1000, 0.003, 0.0001, 0.54],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacked Ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0372 | Test loss: 0.0673: 100%|███████████████████████████████████| 10000/10000 [01:37<00:00, 102.58it/s]\n",
      "Train loss: 0.0260 | Test loss: 0.5068: 100%|███████████████████████████████████| 10000/10000 [01:24<00:00, 117.75it/s]\n",
      "Train loss: 0.0251 | Test loss: 3.7496: 100%|███████████████████████████████████| 10000/10000 [01:34<00:00, 105.66it/s]\n",
      "Train loss: 0.0183 | Test loss: 0.1522: 100%|████████████████████████████████████| 10000/10000 [01:40<00:00, 99.89it/s]\n",
      "Train loss: 0.0379 | Test loss: 0.0428: 100%|███████████████████████████████████| 10000/10000 [01:15<00:00, 133.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "\n",
    "    df = dfe[period[:4]:period[5:]]\n",
    "\n",
    "    pivot_index_training_1 = round(df.shape[0] * 0.25)\n",
    "    pivot_index_training_2 = round(df.shape[0] * 0.75)\n",
    "\n",
    "    df_train_1 = df.iloc[:pivot_index_training_1]\n",
    "    df_ann = df.iloc[pivot_index_training_1:]\n",
    "\n",
    "    ite_train_1 = DataLoader(df=df_train_1, Y=df_train_1.values, window_size=30, batch_size=df_train_1.shape[0])\n",
    "    for batch in ite_train_1: # Only one batch there\n",
    "        X_train_1,y_train_1 = batch\n",
    "\n",
    "    rf_rgs = RandomForestRegressor(n_estimators=30, max_depth=3)  # RandomForestRegressor(max_features = 10, min_samples_split = 24)   \n",
    "    rf_rgs.fit(X_train_1,y_train_1)\n",
    "\n",
    "    sv_rgs = SVR(kernel='rbf', gamma=params_ml[period][4], epsilon=params_ml[period][5])\n",
    "    sv_rgs.fit(X_train_1,y_train_1)\n",
    "\n",
    "    gb_rgs = GradientBoostingRegressor(learning_rate=0.01, n_estimators=30, max_depth=3)  # GradientBoostingRegressor(learning_rate = 0.003, n_estimators=1479)\n",
    "    gb_rgs.fit(X_train_1,y_train_1)\n",
    "\n",
    "    ite = DataLoader(df=df_ann, Y=df_ann.values, window_size=30, batch_size=df_ann.shape[0])\n",
    "    for batch in ite: # Only one batch there\n",
    "        X,y = batch\n",
    "\n",
    "    df_train_ann = pd.DataFrame(X, index=df_ann.index[30:])\n",
    "    df_train_ann[\"rf\"] = rf_rgs.predict(X)\n",
    "    df_train_ann[\"sv\"] = sv_rgs.predict(X)\n",
    "    df_train_ann[\"gb\"] = gb_rgs.predict(X)\n",
    "\n",
    "    kwargs = dict(\n",
    "        learning_rate = params_stacked[period][0],\n",
    "        period = period,\n",
    "        batch_size = df_train_ann.shape[0],\n",
    "        num_epochs = 10000, \n",
    "        window_size = 30,\n",
    "        weight_decay = 0,\n",
    "        tab = df_train_ann.values,\n",
    "        #l2_lambda = params_stacked[period][1],\n",
    "        y = y,\n",
    "    )\n",
    "\n",
    "    sim = Simulation(**kwargs)\n",
    "    sim.Ann.init_weights(random_normal)\n",
    "    sim.make_dataloaders(pivot_index=pivot_index_training_2-pivot_index_training_1)\n",
    "    sim.train(verbose=1)\n",
    "    sim.Ann.save(\"models/torch/stacked_ann_prop/{}\".format(period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed forward-Ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0457 | Test loss: 0.0574: 100%|███████████████████████████████████| 10000/10000 [01:13<00:00, 136.49it/s]\n",
      "Train loss: 0.0245 | Test loss: 0.7881: 100%|███████████████████████████████████| 10000/10000 [01:31<00:00, 108.91it/s]\n",
      "Train loss: 0.0252 | Test loss: 2.8038: 100%|███████████████████████████████████| 10000/10000 [01:30<00:00, 110.17it/s]\n",
      "Train loss: 0.0189 | Test loss: 0.2278: 100%|███████████████████████████████████| 10000/10000 [01:31<00:00, 109.74it/s]\n",
      "Train loss: 0.0359 | Test loss: 0.0432: 100%|███████████████████████████████████| 10000/10000 [01:30<00:00, 109.90it/s]\n"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "\n",
    "    df = dfe[period[:4]:period[5:]]\n",
    "\n",
    "    pivot_index_training_1 = round(df.shape[0] * 0.25)\n",
    "    pivot_index_training_2 = round(df.shape[0] * 0.75)\n",
    "\n",
    "    df_ann = df.iloc[pivot_index_training_1:]\n",
    "\n",
    "    ite = DataLoader(df=df_ann, Y=df_ann.values, window_size=30, batch_size=df_ann.shape[0])\n",
    "    for batch in ite: # Only one batch there\n",
    "        X,y = batch\n",
    "\n",
    "    kwargs = dict(\n",
    "        learning_rate = params_stacked[period][0],\n",
    "        period = period,\n",
    "        batch_size = len(X),\n",
    "        num_epochs = 10000, \n",
    "        window_size = 30,\n",
    "        weight_decay = 0,\n",
    "        #l2_lambda = params_stacked[period][1],\n",
    "        tab = X,\n",
    "        y = y,\n",
    "    )\n",
    "\n",
    "    sim = Simulation(**kwargs)\n",
    "    sim.Ann.init_weights(random_normal)\n",
    "    sim.make_dataloaders(pivot_index=pivot_index_training_2-pivot_index_training_1)\n",
    "    sim.train(verbose=1)\n",
    "    sim.Ann.save(\"models/torch/feed_forward_ann_prop/{}\".format(period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ann-Garch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0436 | Test loss: 0.0867: 100%|███████████████████████████████████| 10000/10000 [01:29<00:00, 111.25it/s]\n",
      "Train loss: 0.0345 | Test loss: 0.9236: 100%|███████████████████████████████████| 10000/10000 [01:34<00:00, 105.56it/s]\n",
      "Train loss: 0.0357 | Test loss: 1.7842: 100%|███████████████████████████████████| 10000/10000 [01:30<00:00, 111.10it/s]\n",
      "Train loss: 0.0320 | Test loss: 0.1896: 100%|███████████████████████████████████| 10000/10000 [01:35<00:00, 105.07it/s]\n",
      "Train loss: 0.0565 | Test loss: 0.0605: 100%|███████████████████████████████████| 10000/10000 [01:37<00:00, 103.05it/s]\n"
     ]
    }
   ],
   "source": [
    "for period in periods:\n",
    "\n",
    "    df = dfe[period[:4]:period[5:]]\n",
    "    df_garch = dfe_garch[period[:4]:period[5:]]\n",
    "\n",
    "    pivot_index_training_1 = round(df.shape[0] * 0.25)\n",
    "    pivot_index_training_2 = round(df.shape[0] * 0.75)\n",
    "\n",
    "    df_ann = df.iloc[pivot_index_training_1:]\n",
    "    df_ann_garch = df_garch.iloc[pivot_index_training_1:]\n",
    "\n",
    "    ite = DataLoader(df=df_ann, Y=df_ann.values, window_size=30, batch_size=df_ann.shape[0])\n",
    "    for batch in ite: # Only one batch there\n",
    "        X,y = batch\n",
    "\n",
    "    ite_garch = DataLoader(df=df_ann_garch, Y=df_ann_garch.values, window_size=30, batch_size=df_ann_garch.shape[0])\n",
    "    for batch in ite_garch: # Only one batch there\n",
    "        X_garch,y_garch = batch\n",
    "\n",
    "    kwargs = dict(\n",
    "        learning_rate = params_stacked[period][0],\n",
    "        period = period,\n",
    "        batch_size = len(X_garch),\n",
    "        num_epochs = 10000, \n",
    "        window_size = 30,\n",
    "        weight_decay = 0,\n",
    "        #l2_lambda = params_stacked[period][1],\n",
    "        tab = X_garch,\n",
    "        y = y,\n",
    "    )\n",
    "\n",
    "    sim = Simulation(**kwargs)\n",
    "    sim.Ann.init_weights(random_normal)\n",
    "    sim.make_dataloaders(pivot_index=pivot_index_training_2-pivot_index_training_1)\n",
    "    sim.train(verbose=1)\n",
    "    sim.Ann.save(\"models/torch/ann_garch_prop/{}\".format(period))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "c9b04fdcb430474004bc74d2d28d415b40d0bb88a523fdc7d942e25fe1aa22b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
